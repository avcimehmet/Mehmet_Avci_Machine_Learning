{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìù FINAL REPORT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection of the Problem:\n",
    "- I chose car price prediction as a Machine Learning project.\n",
    "- I find creating ML model predictions interesting.\n",
    "\n",
    "### Collecting the Data\n",
    "- I obtained the data from kaggle website.\n",
    "\n",
    "### Preparattion of the Data\n",
    "- I defined the csv file data frame as 'df' and I created a copy of it as 'df_copy'.\n",
    "- The df had to many issues to be handled and I did it step by step.\n",
    "- At the beginning, 5 of 18 columns were numeric. (One of them was already the ID column)\n",
    "- I standardized the column names in the data frame.\n",
    "- I observed all features one by one.\n",
    "- I observed that there are so less frequencies of some Make_models in the data frame and I removed the Make_models less than 50 occurences from the dataframe.\n",
    "- After this, total car brands decreased to 23 from 65 with less than 3% of loss of data.\n",
    "- I saved the Engine_volume and Doors features from string values.\n",
    "- I observed that although four features in the data frame like Levy, Engine_volume, Mileage_km and Doors were numeric values, they were seen as object types. I converted them to numeric values.\n",
    "- By the help of inspecting the ID column, I observed that there were too many duplicated rows (520 rows) in the data frame.\n",
    "- I removed duplications using ID column then I dropped the ID column.\n",
    "- Levy feature was the only missing feature in the data frame. I filled it in two steps.\n",
    "- In the first step, for a Make_model with missing Levy value I got the median values of the same Make_models in the dataframe then I filled the missing values.\n",
    "- But some of them could not be filled because in the same Make_model category there were no Levy values to calculate the median.\n",
    "- For this type of missing Levy values I got the median values of the same Make then filled the missing values.\n",
    "- Then no missing values left in the data frame.\n",
    "- To make 1390 unique Make_models in the data frame and make it more stable I removed Make_models which have less than 2 occurences from the data frame.\n",
    "- This costed me a 4.5% of loss of data but total car Make number decreased to 23 from 65. And 1390 Make_model value count decreased to 413.\n",
    "\n",
    "### Exploratory Data Analysis (EDA)\n",
    "- I observed from the pairplot that there was less linear relationship among the numeric features.\n",
    "- This gave me a signal that it would be difficult for a linear model to get better results.\n",
    "- I observed that the target feature (Price) had highly extreme values and outliers within a high range.\n",
    "- So, I calculeted the skewness of the price using Log Transformation and Box-Cox Transformation method on Price feature.\n",
    "- This gave me a more reliable left skewness result. And I visualized it.\n",
    "- I printed the heatmap of the numeric values and I re-printed it after feature engineering.\n",
    "\n",
    "### feature Engineering\n",
    "- I used Tuckey Fences to eliminate the outliers in my target Price feature.\n",
    "- After that I printed the heatmap and I saw that Age and Engine_volume features had remarkable correlation with the Price feature compared to other features.\n",
    "\n",
    "### Data Preprocessing\n",
    "- I observed the object type features in the data frame.\n",
    "- I transformed object type features to numeric features using one-hot encoding and label encoding methods to get the data prepared for the model.\n",
    "\n",
    "### Model Training\n",
    "- I created X and y tables.\n",
    "- I removed 'Price', 'Price_log', 'Levy_percentage' features from X table and added price feature to y table for using in the models.\n",
    "- To avoid the data leakage or overfitting, it was an important point to remove the Price_log and the Levy_percentage features which were calculated by using Price feature.\n",
    "- I splitted the data as Train and Test categories.\n",
    "##### Linear Regression Model\n",
    "- I prepared the data for Linear regression model as X_train, y_train, X_test and y_test categories.\n",
    "- I implemented the Linear Regression and saw that the results with R2 score 52% train and 44% test are not so satisfactory.\n",
    "- Then I implemented cross validation to be sure about the results and saw that I get 45% of median test result from the 10 fold Linear Regression cross validation.\n",
    "##### Random Forest Model\n",
    "- I prepared the data for Random Forest model as X_train, y_train, X_test and y_test categories.\n",
    "- I implemented the Random Forest and saw that the results with R2 score 95% train and 75% test are quite better than the Linear Regression model results.\n",
    "- But the gap between train and test results were significant and it could stem from an overfitting.\n",
    "- To be sure about that I observed the top features of the Random Forest model and saw that there are reasonable features on the top of the list.\n",
    "- To improve the model performance, to tune the hyperparameters and get better results from the Random Forest Regressor model I used Grid search method.\n",
    "- Then I found the best values for 'max_depth', 'min_samples_leaf', 'min_samples_split' and 'n_estimators' parameters.\n",
    "- After implementing these parameters to the model I observed that the result I got initially form the Random Forest Regressor model is consistent with the results after hyperparameter tuning.\n",
    "- Then I implemented cross validation to be sure about the results and saw that I get 76% of median test result from the 10 fold Random Forest Regression cross validation.\n",
    "- Then I visualised the output of the actual and predicted price distribution of the Random Forest model.\n",
    "- Top features, hyperparameter tuning and the cross-validation results proved that the result of the Random forest regresion is reliable and consistent.\n",
    "- Overall, based on the obtained consistent outputs, there isn't clear evidence of overfitting or data leakage.\n",
    "### Compoarison of the Models\n",
    "- I compared the Linear Regression and the Random Forest Regresion models under metrics, scatter plot chart and feature importance titles.\n",
    "- Metrics that I obtained from the comparison both proved the reliability of initial outputs for thr models and demonstrated that Random Forest Regression model performed better than the Linear Regression model.\n",
    "- According to the scatter plot chart it is clearly understood that Random Forest Regression model predictions are closer to the ideal prediction line than the output of the Linear Regression model.\n",
    "- Feature importance comparison demonstrates that the top features of the Random Forest Regression model is more reasonable than the top features of the Lineer Regression model.\n",
    "- In this case, my champion model is the Random Forest Regression model."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
